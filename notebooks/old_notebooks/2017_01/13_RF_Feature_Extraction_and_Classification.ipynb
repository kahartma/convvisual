{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "log = logging.getLogger()\n",
    "log.setLevel(\"DEBUG\")\n",
    "from braindecode.scripts.train_experiments import setup_logging\n",
    "setup_logging()\n",
    "\n",
    "from braindecode.veganlasagne.layer_util import print_layers\n",
    "\n",
    "import os\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import receptive_field\n",
    "import analysis\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelpath = '/home/hartmank/braindecode/data/models/'\n",
    "modelname = 'paper/ours/cnt/deep4/car/22'\n",
    "savepath  = '/home/hartmank/data/convvisual/RF_data/'\n",
    "filename = 'Layer28_nUnits30_nFilters04_filterdiff.data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%cd /home/hartmank/braindecode\n",
    "exp,model,datasets = utils.get_dataset(os.path.join(modelpath,modelname))\n",
    "%cd /home/hartmank/braindecode/convvisual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RF_save = receptive_field.ReceptiveFieldInputsIO()\n",
    "RF_save = RF_save.load(os.path.join(savepath,modelname,filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "cl = 0\n",
    "filt = 0\n",
    "\n",
    "n_chans = RF_save.results[cl].n_chans\n",
    "sampling_rate = RF_save.results[cl].sampling_rate\n",
    "print RF_save.results[cl].max_filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_RF_cropped = utils.get_cropped_RF(RF_save.results[cl].RF_X,([0],[1],n_chans,-1))[filt]\n",
    "window_indeces = np.unique(RF_save.results[cl].max_units_in_filters[filt][:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classes = RF_save.classes\n",
    "neg_classes = np.delete(classes,cl)\n",
    "\n",
    "inputs_baseline = np.array([])\n",
    "for c in neg_classes:\n",
    "    inputs_baseline = np.vstack([inputs_baseline,RF_save.results[c].inputs]) if inputs_baseline.size else RF_save.results[c].inputs\n",
    "X_baseline = utils.cut_ind_windows(inputs_baseline,X_RF_cropped.shape[2],window_indeces).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neg_classes = [cl]\n",
    "\n",
    "inputs_baseline = np.array([])\n",
    "for c in neg_classes:\n",
    "    inputs_baseline = np.vstack([inputs_baseline,RF_save.results[c].inputs]) if inputs_baseline.size else RF_save.results[c].inputs\n",
    "X_baseline_same = utils.cut_ind_windows(inputs_baseline,X_RF_cropped.shape[2],window_indeces).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_baseline = utils.cut_rand_windows(inputs_baseline,X_RF_cropped.shape[2],5).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7200, 128, 522)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_baseline.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#feature_funcs = (analysis.get_frequency,analysis.get_frequency_change,analysis.get_offset,analysis.get_offset_change)\n",
    "\n",
    "#FFT,FFTc,mean,meanc = utils.get_feature_vals(X_RF_cropped,feature_funcs,sampling_rate=sampling_rate)\n",
    "#FFT_base,FFTc_base,mean_base,meanc_base = utils.get_feature_vals(X_baseline,feature_funcs,sampling_rate=sampling_rate)\n",
    "\n",
    "#freqs_FFT = FFT.T[2][0]\n",
    "#freqs_FFTc = FFTc.T[2][0]\n",
    "\n",
    "#FFT = np.asarray(FFT.T[0].tolist())\n",
    "#FFTc = np.asarray(FFTc.T[0].tolist())\n",
    "#FFT_base = np.asarray(FFT_base.T[0].tolist())\n",
    "#FFTc_base = np.asarray(FFTc_base.T[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_mean_diff_same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#feat_means,feat_stds),b,names = analysis.feature_distributions([FFT,FFTc,mean,meanc],['FFT','FFTc','Mean','Meanc'])\n",
    "#feat_stds = np.asarray(feat_stds)\n",
    "#feat_means = np.asarray(feat_means)\n",
    "\n",
    "#feat_means_base,feat_stds_base),b,names = analysis.feature_distributions([FFT_base,FFTc_base,mean_base,meanc_base],['FFT','FFTc','Mean','Meanc'])\n",
    "#feat_stds_base = np.asarray(feat_stds_base)\n",
    "#feat_means_base = np.asarray(feat_means_base)\n",
    "\n",
    "#names = np.asarray(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make sure this is tested!!\n",
      "a (30, 128, 261)\n",
      "a (30, 128, 130)\n",
      "a (30, 128)\n",
      "a (30, 128)\n",
      "Make sure this is tested!!\n",
      "a (2400, 128, 261)\n",
      "a (2400, 128, 130)\n",
      "a (2400, 128)\n",
      "a (2400, 128)\n",
      "Make sure this is tested!!\n",
      "a (30, 128, 261)\n",
      "a (30, 128, 130)\n",
      "a (30, 128)\n",
      "a (30, 128)\n",
      "Make sure this is tested!!\n",
      "a (7200, 128, 261)\n",
      "a (7200, 128, 130)\n",
      "a (7200, 128)\n",
      "a (7200, 128)\n"
     ]
    }
   ],
   "source": [
    "feat_mean_diff_same,names,(FFT,FFTc,mean,meanc),(FFT_base,FFTc_base,mean_base,meanc_base) = utils.extract_fft_and_mean_diff(X_RF_cropped,X_baseline_same,sampling_rate)\n",
    "feat_mean_diff,names,(FFT,FFTc,mean,meanc),(FFT_base,FFTc_base,mean_base,meanc_base) = utils.extract_fft_and_mean_diff(X_RF_cropped,X_baseline,sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#feat_mean_diff = np.abs(np.divide(feat_means_base-feat_means,feat_stds_base))\n",
    "#feat_mean_diff = np.abs(np.divide(feat_means-feat_means_base,feat_stds))\n",
    "feat_mean_diff_diff = feat_mean_diff-feat_mean_diff_same\n",
    "sort_mean_diff_diff = feat_mean_diff_diff.argsort()[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FFT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-17f158a667cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mFFT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'FFT' is not defined"
     ]
    }
   ],
   "source": [
    "FFT.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.6518835   1.32180992  1.24547024  1.23272172  1.22792143  1.22720923\n",
      "  1.20022526  1.15127701  1.07916912  1.03788072  1.03577958  1.00316627\n",
      "  0.99945432  0.99682454  0.98587314  0.96815449  0.96492429  0.91086669\n",
      "  0.89840733  0.8879195   0.88561538  0.88080151  0.85683082  0.85418123\n",
      "  0.84865141  0.84231922  0.84214443  0.83967262  0.83444262  0.82196645]\n",
      "[ 2.31694888  1.70423622  1.26766989  1.54509161  1.41540761  1.32970244\n",
      "  1.81085738  1.39465054  1.16127226  1.45177947  1.36514867  1.35642235\n",
      "  1.40547732  1.04148674  1.10049151  1.41840568  1.08443896  1.36553702\n",
      "  1.38729713  1.02073789  0.91357678  0.91913638  0.89952843  0.9490365\n",
      "  1.01508389  1.33592625  1.16735309  0.94145795  1.1097129   0.90504743]\n",
      "[ 0.66506538  0.3824263   0.02219965  0.31236988  0.18748618  0.1024932\n",
      "  0.61063211  0.24337353  0.08210314  0.41389876  0.32936909  0.35325609\n",
      "  0.40602301  0.0446622   0.11461837  0.45025119  0.11951467  0.45467033\n",
      "  0.4888898   0.13281839  0.0279614   0.03833487  0.04269761  0.09485527\n",
      "  0.16643248  0.49360703  0.32520865  0.10178533  0.27527029  0.08308098]\n"
     ]
    }
   ],
   "source": [
    "print feat_mean_diff_diff[sort_mean_diff_diff[:30]]\n",
    "print feat_mean_diff[sort_mean_diff_diff[:30]]\n",
    "print feat_mean_diff_same[sort_mean_diff_diff[:30]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FFT_55_24', 'FFT_45_24', 'FFT_55_46', 'FFT_34_24', 'Mean_37', 'FFT_55_25', 'FFT_64_24', 'Mean_47', 'FFT_11_24', 'FFT_65_26', 'FFT_72_24', 'FFT_34_25', 'Mean_57', 'FFT_72_25', 'FFT_64_25', 'FFT_73_24', 'FFT_17_24', 'FFT_56_24', 'FFT_65_24', 'FFT_73_25', 'FFT_19_24', 'FFT_34_46', 'FFT_55_22', 'FFT_44_24', 'FFT_26_24', 'Mean_67', 'FFT_55_48', 'FFT_64_48', 'FFT_64_20', 'FFT_44_49']\n"
     ]
    }
   ],
   "source": [
    "print names[sort_mean_diff_diff[:30]].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make sure this is tested!!\n",
      "a (261, 128, 261)\n",
      "a (261, 128, 130)\n",
      "a (261, 128)\n",
      "a (261, 128)\n",
      "Make sure this is tested!!\n",
      "a (260, 128, 261)\n",
      "a (260, 128, 130)\n",
      "a (260, 128)\n",
      "a (260, 128)\n"
     ]
    }
   ],
   "source": [
    "batch_train = utils.get_dataset_batches(exp,datasets['train'],1000,True)[0]\n",
    "inputs,targets = batch_train\n",
    "targets = targets.reshape((len(inputs),-1,4))\n",
    "targets = targets.sum(axis=1).argmax(axis=1)\n",
    "inputs = utils.cut_rand_windows(inputs,X_RF_cropped.shape[2],1).squeeze()\n",
    "feat_mean_diff_same,names,(FFT_tr,FFTc_tr,mean_tr,meanc_tr),(FFT_base_tr,FFTc_base_tr,mean_base_tr,meanc_base_tr) = utils.extract_fft_and_mean_diff(inputs[targets==cl],inputs[targets!=cl][:260],sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make sure this is tested!!\n",
      "a (261, 128, 261)\n",
      "a (261, 128, 130)\n",
      "a (261, 128)\n",
      "a (261, 128)\n",
      "Make sure this is tested!!\n",
      "a (260, 128, 261)\n",
      "a (260, 128, 130)\n",
      "a (260, 128)\n",
      "a (260, 128)\n"
     ]
    }
   ],
   "source": [
    "batch_valid = utils.get_dataset_batches(exp,datasets['valid'],1000,True)[0]\n",
    "inputs,targets = batch_train\n",
    "targets = targets.reshape((len(inputs),-1,4))\n",
    "targets = targets.sum(axis=1).argmax(axis=1)\n",
    "inputs = utils.cut_rand_windows(inputs,X_RF_cropped.shape[2],1).squeeze()\n",
    "feat_mean_diff_same,names,(FFT_v,FFTc_v,mean_v,meanc_v),(FFT_base_v,FFTc_base_v,mean_base_v,meanc_base_v) = utils.extract_fft_and_mean_diff(inputs[targets==cl],inputs[targets!=cl][:260],sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FFT_channs = [55,45,55,34,55,64,11,65,72,34,72,64,73,17,56,65,73,19,34,55,44,26,55,64,64,44]\n",
    "FFT_freqs = [24,24,46,24,25,24,24,26,24,25,25,25,24,24,24,24,25,24,46,22,24,24,48,48,20,49]\n",
    "mean_channs = [37,47,57,67]\n",
    "\n",
    "X_FFT = FFT_tr[:,FFT_channs,FFT_freqs]\n",
    "X_mean = mean_tr[:,mean_channs]\n",
    "X_class = np.hstack((X_FFT,X_mean))\n",
    "y_class = np.ones((X_class.shape[0],1))\n",
    "\n",
    "X_FFT = FFT_base_tr[:,FFT_channs,FFT_freqs]\n",
    "X_mean = mean_base_tr[:,mean_channs]\n",
    "X_base = np.hstack((X_FFT,X_mean))\n",
    "y_base = np.zeros((X_base.shape[0],1))\n",
    "\n",
    "X = np.vstack((X_class,X_base))\n",
    "y = np.vstack((y_class,y_base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.svm import LinearSVC as SVC\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.neural_network import MLPClassifier as MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage='auto',\n",
       "              solver='lsqr', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LDA(solver='lsqr', shrinkage='auto')\n",
    "#clf = SVC()\n",
    "#clf = LR()\n",
    "#clf = MLP()\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out = clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85604606525911708"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(out==y.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  0.,  0.,  1.,  1.,  0.,  1.,  1.,  1.,  0.,  0.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  0.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  0.,  1.,  0.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,  0.,\n",
       "        0.,  0.,  1.,  1.,  0.,  0.,  1.,  1.,  0.,  1.,  1.,  1.,  0.,\n",
       "        0.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,\n",
       "        1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        0.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  0.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,  0.,\n",
       "        1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,  0.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,\n",
       "        0.,  1.,  0.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,  0.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  0.,  1.,  1.,  0.,  1.,  0.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_FFT = FFT_v[:,FFT_channs,FFT_freqs]\n",
    "X_mean = mean_v[:,mean_channs]\n",
    "X_class = np.hstack((X_FFT,X_mean))\n",
    "y_class = np.ones((X_class.shape[0],1))\n",
    "\n",
    "X_FFT = FFT_base_v[:,FFT_channs,FFT_freqs]\n",
    "X_mean = mean_base_v[:,mean_channs]\n",
    "X_base = np.hstack((X_FFT,X_mean))\n",
    "y_base = np.zeros((X_base.shape[0],1))\n",
    "\n",
    "X = np.vstack((X_class,X_base))\n",
    "y = np.vstack((y_class,y_base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out = clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50095969289827258"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(out==y.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print np.mean(out[np.where(y.T==1)[1]]==1)\n",
    "print np.mean(out[np.where(y.T==0)[1]]==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 128, 261)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FFT.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-01-13 16:23:35,425 Setting n_sample preds automatically to 479\n",
      "2017-01-13 16:23:35,437 Input window length is 522\n",
      "2017-01-13 16:23:35,789 Setting n_sample preds automatically to 479\n",
      "2017-01-13 16:23:35,792 Input window length is 522\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hartmank/braindecode\n",
      "2017-01-13 16:40:46,362 Setting n_sample preds automatically to 479\n",
      "2017-01-13 16:40:46,365 Input window length is 522\n",
      "2017-01-13 16:40:46,893 Setting n_sample preds automatically to 479\n",
      "2017-01-13 16:40:46,897 Input window length is 522\n",
      "2017-01-13 16:40:46,900 Load Training Set...\n",
      "2017-01-13 16:41:18,710 Load Test Set...\n",
      "2017-01-13 16:41:24,572 Clean Training Set...\n",
      "2017-01-13 16:41:25,918 Rejected channels: []\n",
      "2017-01-13 16:41:25,920 #Clean trials:     895\n",
      "2017-01-13 16:41:25,922 #Rejected trials:  2\n",
      "2017-01-13 16:41:25,924 Fraction Clean:    99.0%\n",
      "2017-01-13 16:41:25,925 (from maxmin):     2\n",
      "2017-01-13 16:41:25,927 (from var):        0\n",
      "2017-01-13 16:41:26,248 Clean Test Set...\n",
      "2017-01-13 16:41:26,410 Rejected channels: []\n",
      "2017-01-13 16:41:26,412 #Clean trials:     160\n",
      "2017-01-13 16:41:26,413 #Rejected trials:  0\n",
      "2017-01-13 16:41:26,415 Fraction Clean:    100.0%\n",
      "2017-01-13 16:41:26,416 (from maxmin):     0\n",
      "2017-01-13 16:41:26,418 (from var):        0\n",
      "2017-01-13 16:41:26,419 Create Cleaned Cnt Sets...\n",
      "2017-01-13 16:41:28,987 Create sets from cleaned cnt...\n",
      "2017-01-13 16:41:28,988 Preprocess continuous signal...\n",
      "2017-01-13 16:42:36,738 Loaded dataset with shape: (1725659, 128, 1, 1)\n",
      "2017-01-13 16:42:36,743 Preprocess continuous signal...\n",
      "2017-01-13 16:42:48,252 Loaded dataset with shape: (308544, 128, 1, 1)\n",
      "2017-01-13 16:42:48,253 Loaded clean train data with shape (1725659, 128, 1, 1).\n",
      "2017-01-13 16:42:48,253 Loaded clean test data with shape (308544, 128, 1, 1).\n",
      "/home/hartmank/braindecode/convvisual\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batches_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ReceptiveFieldInputsIO instance has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-973a3f78ced4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mRF_save\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: ReceptiveFieldInputsIO instance has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "RF_save.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
